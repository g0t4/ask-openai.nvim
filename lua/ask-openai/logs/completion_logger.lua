local log = require('ask-openai.logs.logger').predictions()

local M = {}

---@param sse_parsed table
---@param request CurlRequest
function M.log_full_request(sse_parsed, request, frontend)
    log:error("TODO FINISH LOGGING COMPLETIONS LATER")
    do return end

    -- TODO GET CONTENT/RESPONSE TOO (IIRC this is frontend specific?...
    --   TODO how about add a mechanism to track the deltas on the request object too?)
    --  b/c you want to save the output message that you parsed too (content + reasoning) along with this
    --   would be nice to get the content again in the final response? raw prompt? I think there might be a setting for that?
    --      in __verbose.content ... it isn't set on streaming requests AFAICT... but there is a toggle IIRC (maybe)

    -- FYI
    local id = sse_parsed.id

    -- log:error("request ID", id) -- generated by llama-server => chatcmpl-s1530j4cdcNyQcLvEz2hXtlaNznHxp3z
    -- let's do it by time so I can easily find a specific request based on timeframe
    --   and purge them too that way
    log:error("full sse", vim.inspect(sse_parsed))
    log:error("full request", vim.inspect(request)) -- for reasoning+ content (parsed output message) + body (input-messages)
    log:error("input-messages.json", vim.inspect(request.body)) -- keep this to reproduce issues with template rendering
    log:error("input-prompt.json", vim.inspect(sse_parsed.__verbose.prompt)) -- this is the raw prompt into the model

    -- FYI if stream=false, then the last SSE has .__verbose.content (but not for streaming)
end

return M
